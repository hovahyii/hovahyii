"use client";

import Image from 'next/image';

export default function GLM5Content() {
    return (
        <div className="prose lg:prose-xl max-w-none">
            <div className="mb-8 p-6 rounded-2xl border border-blue-200 dark:border-blue-800 bg-gradient-to-br from-blue-50 via-white to-slate-50 dark:from-blue-950/40 dark:via-slate-900 dark:to-slate-900">
                <p className="text-sm uppercase tracking-[0.3em] text-blue-600 dark:text-blue-300 font-semibold">AI BREAKING NEWS ‚Ä¢ 2026-02-07</p>
                <h2 className="text-3xl font-extrabold mt-3 mb-4 text-slate-900 dark:text-white">üëë New King of Open Source: Zhipu GLM-5 Released, Coding Capabilities Rival Claude Opus 4.5</h2>
                <p className="text-lg text-slate-700 dark:text-slate-200">
                    Late on February 7th, a mysterious model codenamed "Pony Alpha" quietly went online. Throw in some "spaghetti code" that took a day to fix, and it refactors the architecture effortlessly; input a simple prompt, and it spits out a complete Web App with 35 radio stations and a silky-smooth UI.
                </p>
            </div>

            <p>
                This extreme engineering capability directly confirms Andrej Karpathy's assertion a few days ago: Vibe Coding is a thing of the past, and the new game rule has only one name‚Äî‚Äî
            </p>
            <p className="text-xl font-bold text-center my-8">
                Agentic Engineering.
            </p>

            <div className="my-8 flex justify-center">
                <Image
                    src="/blog/glm5.0/andrej-karpathy.png"
                    alt="Andrej Karpathy"
                    width={900}
                    height={500}
                    className="rounded-2xl shadow-lg"
                />
            </div>

            <p>
                Following this, Opus 4.6 and GPT-5.3-Codex "crashed" the party late the next day, with full reports discussing nothing but "long-horizon tasks and systems engineering."
            </p>
            <p>
                Just when everyone thought this was another solo act for closed-source giants, the mystery of Pony Alpha was revealed‚Äî‚Äî
            </p>
            <p className="font-bold text-lg">It is GLM-5.</p>
            <p>
                The world's first open-source model to stand on this track and go head-to-head with Silicon Valley giants in system-level engineering capabilities.
            </p>
            <p>
                After the reveal, Zhipu's stock price skyrocketed by 32%!
            </p>

            <div className="my-8 flex justify-center">
                <Image
                    src="/blog/glm5.0/atlas.png"
                    alt="GLM-5 Reveal"
                    width={900}
                    height={500}
                    className="rounded-2xl shadow-lg"
                />
            </div>

            <h3 className="text-2xl font-bold mt-12 mb-6">Global Open Source #1! The "Opus Moment" for Domestic Models</h3>
            <p>
                After actually getting our hands on it, we only have one feeling: it's truly powerful!
            </p>
            <p>
                If Claude Opus represented the peak of closed-source models, then the release of GLM-5 undoubtedly marks the arrival of the "Opus Moment" for domestic open-source models.
            </p>

            <div className="my-8 flex justify-center">
                <Image
                    src="/blog/glm5.0/intelligence index.png"
                    alt="Artificial Analysis Ranking"
                    width={900}
                    height={500}
                    className="rounded-xl shadow"
                />
            </div>
            <p className="text-sm text-center text-slate-500 mt-2">In the authoritative Artificial Analysis list, GLM-5 ranks fourth globally and first in open source.</p>

            <p>
                On the day of release, more than 10 games and tools "hand-rubbed" by developers based on GLM-5 were exhibited and available for experience, and these applications will subsequently land on major app stores.
            </p>
            <p>
                This means GLM-5 is transforming "AI Programming" into "AI Delivery," truly realizing the seamless leap from productivity tools to commercial products.
            </p>

            <div className="my-8 flex justify-center">
                <Image
                    src="/blog/glm5.0/showcase.png"
                    alt="GLM-5 Showcase"
                    width={900}
                    height={500}
                    className="rounded-xl shadow"
                />
            </div>
            <p className="text-center mb-6"><a href="https://showcase.z.ai" target="_blank" className="text-blue-600 hover:underline">Experience here: showcase.z.ai</a></p>

            <p>
                Take for example the project named "Pookie World."
            </p>
            <p>
                It is a digital parallel world driven by GLM-5, imparting real narrative integrity and life motivation to autonomous agents through a multi-layer bio-psychological framework.
            </p>

            <div className="my-8 flex justify-center">
                <Image
                    src="/blog/glm5.0/new resident.gif"
                    alt="Pookie World"
                    width={900}
                    height={500}
                    className="rounded-xl shadow"
                    unoptimized
                />
            </div>

            <p>
                There's also a replica of "Minecraft," where the effects and gameplay are almost exactly the same as the original.
            </p>

            <div className="my-8 flex justify-center">
                <Image
                    src="/blog/glm5.0/minecraft.png"
                    alt="Minecraft Clone"
                    width={900}
                    height={500}
                    className="rounded-xl shadow"
                />
            </div>

            <p>
                We also used Claude Code as a shell to directly access GLM-5's API for multi-dimensional testing.
            </p>
            <p>
                Whether it's a Next.js full-stack project or a MacOS/iOS native application, it can achieve a closed loop from requirements analysis and architecture design to coding and end-to-end debugging.
            </p>
            <p>
                After doing N projects, there is a feeling in the air:
            </p>
            <p className="font-semibold">
                To some extent, GLM-5 might be a model capable of changing the industry landscape.
            </p>

            <div className="my-8 flex justify-center">
                <Image
                    src="/blog/glm5.0/claude code.png"
                    alt="Claude Code with GLM-5"
                    width={900}
                    height={500}
                    className="rounded-xl shadow"
                />
            </div>

            <h3 className="text-2xl font-bold mt-12 mb-6">Complex Logic Challenge: "Infinite Knowledge Universe"</h3>
            <p>
                If you think writing a webpage is simple, try letting AI handle an "Infinite Flow" project with strict JSON format requirements involving dynamic rendering.
            </p>
            <p>
                Take the "Infinite Knowledge Universe" we tested first.
            </p>
            <p>
                This is a typical complex front-end and back-end separation project, involving React Flow dynamic rendering, Next.js API route design, and extremely strict JSON format output requirements.
            </p>
            <p>
                GLM-5's performance in this regard was stunning.
            </p>

            <div className="my-8 flex justify-center">
                <Image
                    src="/blog/glm5.0/diagram.gif"
                    alt="Infinite Knowledge Universe Demo"
                    width={900}
                    height={500}
                    className="rounded-xl shadow"
                    unoptimized
                />
            </div>

            <p>
                Not only did it complete the entire project file structure in one go, but its debug logic was surprisingly good.
            </p>

            <div className="grid md:grid-cols-2 gap-4 my-8">
                <Image
                    src="/blog/glm5.0/tree.png"
                    alt="Project Structure"
                    width={450}
                    height={300}
                    className="rounded-lg shadow"
                />
                <Image
                    src="/blog/glm5.0/structure.png"
                    alt="Code Structure"
                    width={450}
                    height={300}
                    className="rounded-lg shadow"
                />
            </div>

            <p>
                When encountering a rendering bug, we only said, "The page is still black, the first content didn't appear during initialization..."
            </p>
            <p>
                GLM-5 immediately located the loading timing issue and quickly provided a fix.
            </p>

            <div className="my-8 flex justify-center">
                <Image
                    src="/blog/glm5.0/document.png"
                    alt="Debugging"
                    width={900}
                    height={500}
                    className="rounded-xl shadow"
                />
            </div>

            <div className="bg-slate-100 dark:bg-slate-800 p-6 rounded-lg my-6">
                <h4 className="font-bold mb-4">Complete Prompt Ref:</h4>
                <p className="font-semibold">Infinite Flow ¬∑ Concept Visualization</p>
                <ul className="list-disc pl-5 mt-2 space-y-2">
                    <li><strong>Core Concept:</strong> An "endless" mind map. Users input any keyword (like "Quantum Physics" or "Dream of the Red Chamber"), and the system generates a central node. Clicking any node triggers AI to expand its child nodes in real-time.</li>
                    <li><strong>Wow Moment:</strong> Users feel like they are interacting with an omniscient brain. When they randomly click on an obscure concept, the AI still accurately expands the next level, creating a shocking sense of "infinite exploration."</li>
                    <li><strong>Visuals & Viral:</strong> Use React Flow or ECharts for dynamic, draggable node networks. Use Cyberpunk or Minimalist color schemes, perfect for screenshots.</li>
                    <li><strong>Feasibility:</strong>
                        <ul className="list-circle pl-5 mt-1">
                            <li>Frontend: React + React Flow.</li>
                            <li>Backend: Next.js API Route.</li>
                            <li>Prompt Strategy: No complex context memory needed, just ask AI to generate 5-6 related child nodes for the "current node" and return JSON.</li>
                            <li>Hard Part: Ensuring stable JSON output (a great test for instruction following).</li>
                        </ul>
                    </li>
                </ul>
            </div>

            <h3 className="text-2xl font-bold mt-12 mb-6">More Complex Middleware Project: Built in 11 Minutes</h3>
            <p>
                Next, we increased the difficulty and asked it to develop a psychological analysis app called "Soul Mirror."
            </p>
            <p>
                The requirements were divided into two steps:
            </p>
            <ul className="list-disc pl-6 mb-6 text-lg">
                <li><strong>Step 1 Logic Design:</strong> Act as a Jungian psychology expert, outputting JSON containing analysis text and visual parameters.</li>
                <li><strong>Step 2 Frontend Implementation:</strong> Dynamically render SVGs based on parameters to generate Tarot-style cards.</li>
            </ul>

            <div className="bg-slate-100 dark:bg-slate-800 p-6 rounded-lg my-6">
                <h4 className="font-bold mb-4">Prompt Ref:</h4>
                <p className="font-semibold">Step 1: Logic Design</p>
                <p>We want to develop a "Soul Mirror" app. Flow: 1. Intro: User inputs state/confusion. 2. Analysis: AI asks 2 deep questions. 3. Result: AI generates a "Soul Card".</p>
                <p>Design the Core Prompt (System Instruction): Act as a Jungian expert. Finally, output a JSON with: `analysis` (text) and `visualParams` (colorPalette, shapes, chaosLevel).</p>
                <br />
                <p className="font-semibold">Step 2: Frontend & SVG</p>
                <p>Write Next.js code. Focus on a `ResultCard` component. 1. Receive `visualParams`. 2. Draw dynamic SVG (high chaos = irregular paths, warm colors = gradients). 3. Tarot-style layout. 4. "Save as Image" button.</p>
            </div>

            <p>
                Throughout the process, its understanding capabilities often made us suspect we were using Opus 4.5.
            </p>
            <p>
                But a glance confirmed it was indeed GLM-5.
            </p>

            <div className="my-8 flex justify-center">
                <Image
                    src="/blog/glm5.0/start.png"
                    alt="Soul Mirror Start"
                    width={900}
                    height={500}
                    className="rounded-xl shadow"
                />
            </div>

            <h3 className="text-2xl font-bold mt-12 mb-6">25 Minutes One-Take, True Agentic Coding</h3>
            <p>
                To further test GLM-5's capabilities, we asked it not to use APIs but to completely simulate a real user to create an X platform monitoring system.
            </p>
            <p>
                Result: 25 minutes, one continuous take.
            </p>
            <p>
                GLM-5 autonomously called various tool agents during the run, planned tasks, broke down steps, and checked documentation to fix errors when encountered.
            </p>
            <p>
                This ability to maintain logical consistency over a long period was unimaginable for previous open-source models.
            </p>

            <div className="my-8 flex justify-center">
                <Image
                    src="/blog/glm5.0/autocorrect.png"
                    alt="Auto Correction"
                    width={900}
                    height={500}
                    className="rounded-xl shadow"
                />
            </div>
            <p className="text-center text-sm text-slate-500 mt-2">Completed, GLM-5 automatically runs the project with one sentence.</p>

            <h3 className="text-2xl font-bold mt-12 mb-6">Image to App: The Fidelity is Incredible</h3>
            <p>
                Finally, we took a screenshot of an open-source project from the father of OpenClaw (an AI quota statistics tool) and threw it directly to GLM-5:
            </p>
            <p className="italic">"Make me a MacOS App based on this."</p>

            <div className="my-8 flex justify-center">
                <Image
                    src="/blog/glm5.0/gemini.png"
                    alt="Image Input"
                    width={900}
                    height={500}
                    className="rounded-xl shadow"
                />
            </div>

            <p>
                In a short while, it really "replicated" an identical product.
            </p>
            <p>
                Although the data is mocked, the UI layout and interaction logic are almost perfect replicas.
            </p>
            <p>
                This is not just visual understanding, but the engineering landing capability of converting vision into SwiftUI code.
            </p>

            <div className="my-8 flex justify-center">
                <Image
                    src="/blog/glm5.0/cursor.png"
                    alt="Replicated App"
                    width={900}
                    height={500}
                    className="rounded-xl shadow"
                />
            </div>

            <h3 className="text-2xl font-bold mt-12 mb-6">God-Tier Hand-Rubbing: 1 Day to Replicate "Budget Cursor"</h3>
            <p>
                To verify GLM-5's engineering limit, a senior developer decided to play it big:
            </p>
            <p>
                Hand-rubbing an AI programming assistant with a desktop UI from scratch‚Äî‚ÄîGLMLIFE.
            </p>
            <p>
                This is equivalent to making a simple version of Cursor.
            </p>

            <div className="my-8 flex justify-center">
                <Image
                    src="/blog/glm5.0/vscode.png"
                    alt="GLMLIFE Concept"
                    width={900}
                    height={500}
                    className="rounded-xl shadow"
                />
            </div>

            <p>
                After throwing the task to GLM-5, it didn't start writing code frantically but first threw out a professional architecture design document (PLAN.md) and made extremely mature technology choices:
            </p>
            <p>
                It directly adopted the Monorepo architecture and precisely disassembled the project into three core packages.
            </p>
            <ul className="list-disc pl-6 mb-6">
                <li><strong>Core:</strong> Responsible for Agent core engine and LLM adaptation;</li>
                <li><strong>CLI:</strong> Handles command-line interaction;</li>
                <li><strong>Desktop:</strong> Desktop main program based on Electron + React 18.</li>
            </ul>

            <div className="my-8 flex justify-center">
                <Image
                    src="/blog/glm5.0/glmlife.png"
                    alt="GLMLIFE Architecture"
                    width={900}
                    height={500}
                    className="rounded-xl shadow"
                />
            </div>

            <p>
                From Zustand state management to Tailwind style schemes, and then to complex IPC process communication, GLM-5 is like a technical director with ten years of team experience, arranging technical choices clearly.
            </p>
            <p>
                The developer originally thought it would take three days to configure the environment, but it only took one day to run through the entire process from environment setup, core logic implementation to Electron packaging.
            </p>
            <p>
                Opening GLMLIFE, it's hard to believe this is a product "architected" by AI in one day.
            </p>

            <div className="my-8 flex justify-center">
                <Image
                    src="/blog/glm5.0/running.gif"
                    alt="GLMLIFE Running"
                    width={900}
                    height={500}
                    className="rounded-xl shadow"
                    unoptimized
                />
            </div>
            <p className="text-center text-sm text-slate-500 mt-2">GLMLIFE making a mini piano implementation process</p>

            <h3 className="text-2xl font-bold mt-12 mb-6">Why Can It Become the "Opus of the Open Source World"?</h3>
            <p>
                Looking globally, Claude Opus 4.6 and GPT-5.3-Codex are sought after because they possess extremely strong "architectural" capabilities.
            </p>
            <ul className="list-disc pl-6 mb-6">
                <li><strong>Opus 4.6's Brute Force Aesthetics:</strong> 16 AI avatars autonomously divided labor, taking two weeks to build a Rust compiler containing 100,000 lines of code from scratch, passing 99% of GCC stress tests.</li>
                <li><strong>GPT-5.3's Self-Creation:</strong> It is OpenAI's first model to "participate in its own creation," intervening in the training process and cluster deployment before "birth."</li>
            </ul>
            <p>
                However, all this has a fatal premise: they are not only closed-source but also expensive.
            </p>
            <p>
                At this moment, the release of GLM-5 is a violent breakthrough by Chinese open-source large models for the Agentic era.
            </p>
            <p>
                It stares directly at the field that closed-source giants least want to let go of‚Äî‚Äîsystem-level engineering capabilities‚Äî‚Äîand launches a "replacement-style" attack.
            </p>

            <h4 className="text-xl font-bold mt-6 mb-4">1. New "Backend Architect"</h4>
            <p>
                The Zhipu team knows very well that the open-source world does not lack models that can write Python scripts; what it lacks is models that can handle dirty work, tiring work, and big work.
            </p>
            <p>
                GLM-5 has significantly strengthened the weight of backend architecture design, complex algorithm implementation, and stubborn Bug fixing in training, and also achieved a strong self-reflection mechanism.
            </p>
            <p>
                When compilation fails, it will analyze logs, locate root causes, modify code, and recompile like a mature engineer until the system runs.
            </p>

            <h4 className="text-xl font-bold mt-6 mb-4">2. Since It's Work, Do the Math</h4>
            <p>
                Performance comparable to Opus, combined with open-source weights, allows GLM-5 to shake the walled gardens built by Anthropic and OpenAI to some extent.
            </p>
            <ul className="list-disc pl-6 mb-6">
                <li><strong>Local Deployment:</strong> It can run not only on a completely isolated intranet but also be fine-tuned for the company's private framework, becoming the specialist who knows its own code best.</li>
                <li><strong>Controllable Cost:</strong> Users can run a powerful Coding Agent on a consumer-grade graphics card cluster without feeling the pinch every time they run a test.</li>
            </ul>

            <h3 className="text-2xl font-bold mt-12 mb-6">Dominating SOTA Leaderboards</h3>
            <p>
                GLM-5's evolution can only be described in one word: brutal.
            </p>
            <p>
                Since it is a base model designed for complex system engineering, the scale must be maxed out.
            </p>
            <p>
                The parameter count went from 355B (active 32B) straight to 744B (active 40B), and pre-training data piled up from 23T to 28.5T.
            </p>
            <p>
                Besides being "big," it also has to be "thrifty."
            </p>
            <p>
                As everyone knows, the most expensive thing about running an Agent is Tokens.
            </p>
            <p>
                To solve this pain point, GLM-5 integrated the DeepSeek Sparse Attention mechanism for the first time.
            </p>
            <p>
                This allows it to maintain "lossless" memory while handling ultra-long contexts, while significantly reducing deployment costs.
            </p>
            <p>
                There's an even more ruthless "black technology"‚Äî‚Äîthe brand new asynchronous reinforcement learning framework Slime.
            </p>
            <p>
                Combined with large-scale reinforcement learning, the model is no longer a "one-time tool" but a "long-distance runner" that gets smarter as it runs.
            </p>
            <p>
                As for benchmarks, it's hardcore:
            </p>
            <p className="font-bold">Code Ability</p>
            <p>
                SWE-bench Verified shot straight to 77.8 points, Terminal Bench 2.0 took 56.2 points in one fell swoop, both No. 1 in open source. This score not only surpassed Gemini 3.0 Pro but also sticks right to Claude Opus 4.5's face.
            </p>

            <div className="my-8 flex justify-center">
                <Image
                    src="/blog/glm5.0/performance.png"
                    alt="Benchmark Performance"
                    width={900}
                    height={500}
                    className="rounded-xl shadow"
                />
            </div>

            <p className="font-bold">Agent Ability</p>
            <p>
                BrowseComp (web retrieval), MCP-Atlas (tool calling), and √è‚Äû√Ç¬≤-Bench (complex planning), all sweeping open source No. 1.
            </p>
            <p>
                Most interesting is Vending Bench 2 (vending machine operation test).
            </p>
            <p>
                In this test, the model has to rely entirely on itself to operate a vending machine for a year.
            </p>
            <p>
                Guess what, GLM-5 actually earned $4432 by the end of the year, a level almost catching up with Opus 4.5.
            </p>

            <div className="my-8 flex justify-center">
                <Image
                    src="/blog/glm5.0/money balance.png"
                    alt="Vending Bench 2 Result"
                    width={900}
                    height={500}
                    className="rounded-xl shadow"
                />
            </div>

            <p>
                In the internal Claude Code evaluation set most cared about by developers, GLM-5 significantly surpassed the previous generation GLM-4.7 on frontend, backend, long-horizon tasks, etc. (average increase over 20%).
            </p>
            <p>
                The real usage feel is already approaching Opus 4.5.
            </p>


            <div className="my-8 flex justify-center">
                <Image
                    src="/blog/glm5.0/cc-bench-v2.png"
                    alt="Claude Code Bench"
                    width={900}
                    height={500}
                    className="rounded-xl shadow"
                />
            </div>

            <h3 className="text-2xl font-bold mt-12 mb-6">Using AI to Build AI</h3>
            <p>
                Of course, GLM-5's ambition is not limited to models, but lies in restructuring the programming tools in our hands.
            </p>
            <p>
                The globally popular OpenClaw has shown everyone the potential of AI operating computers.
            </p>
            <p>
                This time, Zhipu also launched the AutoGLM version of OpenClaw.
            </p>
            <p>
                When using the original version, just environment configuration took half a day, now the official website has one-click deployment.
            </p>
            <p>
                Want a "digital intern" to watch Twitter, organize news, or even write scripts for you 24x7? Click and you have it.
            </p>
            <p>
                Also released is Z Code‚Äî‚Äî
            </p>
            <p>
                A new generation development tool born completely based on GLM-5 capabilities.
            </p>
            <p>
                In Z Code, you just make requests, the model automatically disassembles tasks, and even pulls up a bunch of Agents to work concurrently: writing code, running commands, Debugging, previewing, and finally even handling Git submissions for you.
            </p>
            <p>
                Even more, you can use your mobile phone to remotely command the desktop Agent to work.
            </p>
            <p>
                It is worth mentioning that, just like OpenAI used Codex to write Codex, Z Code itself was also developed with the full participation of GLM models.
            </p>

            <h3 className="text-2xl font-bold mt-12 mb-6">Victory of Domestic Computing Power</h3>
            <p>
                Behind GLM igniting global traffic and the surge in Agent demand, a group of "unsung heroes" are silently supporting massive computing loads.
            </p>
            <p>
                To ensure every line of code and every Agent plan outputs stably, GLM-5 has gone deep into the "hinterland" of domestic computing power, completing deep adaptation with mainstream platforms like Huawei Ascend, Moore Threads, Cambricon, Kunlunxin, MetaX, Enflame, and Haiguang.
            </p>
            <p>
                Through refined optimization at the underlying operator level, GLM-5 can also run "high throughput, low latency" full-blooded performance on domestic chip clusters.
            </p>
            <p>
                This means we not only have top-tier models but are also not being choked.
            </p>

            <h3 className="text-2xl font-bold mt-12 mb-6">Conclusion</h3>
            <p>
                In the spring of 2026, large programming models finally shed their childishness.
            </p>
            <p>
                Karpathy's so-called "Agentic Engineering" is essentially a stricter "interview requirement" for AI:
            </p>
            <ul className="list-disc pl-6 mb-6">
                <li><strong>Previously (Vibe Coding):</strong> As long as you can write beautiful HTML, I'll hire you.</li>
                <li><strong>Now (Agentic Coding):</strong> You need to understand the Linux kernel, understand the call relationships between 500 microservices, know how to refactor code without blowing up production, and be able to plan tasks and fix bugs yourself.</li>
            </ul>
            <p>
                GLM-5 is not perfect.
            </p>
            <p>
                But on the core proposition of "building complex systems," it is currently the only player in the open-source world that can catch this "Agentic Wave."
            </p>
            <p>
                Vibe Coding is over.
            </p>
            <p>
                Stop asking AI "Can you help me write a webpage?" That was a 2025 thing.
            </p>
            <p>
                Now, try asking it: "Can you help me refactor the core module of this high-concurrency system?"
            </p>
            <p className="text-xl font-bold">
                GLM-5, Ready to Build!
            </p>

            <div className="bg-yellow-50 dark:bg-yellow-900/30 p-6 rounded-lg my-8 border border-yellow-200 dark:border-yellow-700">
                <h4 className="text-lg font-bold mb-2">Easter Egg ü•ö</h4>
                <p>GLM-5 has been included in the Max user package, and Pro will support it within 5 days as soon as possible!</p>
                <p>And Zhipu just announced a price increase, this year's Token is destined to be more expensive!</p>
            </div>

            <p className="font-bold text-lg mt-8 mb-4">Go experience it now!</p>
            <p className="font-semibold">Official API Access</p>
            <ul className="list-disc pl-6 mb-4">
                <li>BigModel Open Platform: <a href="https://docs.bigmodel.cn/cn/guide/models/text/glm-5" target="_blank" className="text-blue-600 hover:underline">https://docs.bigmodel.cn/cn/guide/models/text/glm-5</a></li>
                <li>Z.ai: <a href="https://docs.z.ai/guides/llm/glm-5" target="_blank" className="text-blue-600 hover:underline">https://docs.z.ai/guides/llm/glm-5</a></li>
                <li>OpenClaw Access Docs: <a href="https://docs.bigmodel.cn/cn/coding-plan/tool/openclaw" target="_blank" className="text-blue-600 hover:underline">https://docs.bigmodel.cn/cn/coding-plan/tool/openclaw</a></li>
            </ul>

            <p className="font-semibold">Open Source Links</p>
            <ul className="list-disc pl-6 mb-4">
                <li>GitHub: <a href="https://github.com/zai-org/GLM-5" target="_blank" className="text-blue-600 hover:underline">https://github.com/zai-org/GLM-5</a></li>
                <li>Hugging Face: <a href="https://huggingface.co/zai-org/GLM-5" target="_blank" className="text-blue-600 hover:underline">https://huggingface.co/zai-org/GLM-5</a></li>
                <li>ModelScope: <a href="https://modelscope.cn/models/ZhipuAI/GLM-5" target="_blank" className="text-blue-600 hover:underline">https://modelscope.cn/models/ZhipuAI/GLM-5</a></li>
            </ul>

        </div>
    );
}
